- Ajax로 요청한 데이터를 정리해서 출력 / 마무리
- 빅데이터 플랫폼 구축
	1) VMWare설치
	2) 네트워크를 위한 설정 파일 복사
		(vmnetcfg.exe - player버전과 vmnetcfg.exe를 copy한 workstation pro버전이 동일해야 한다.)
	3) CentOS 설치
		언어 설정 / 키보드 영어 추가 / 소프트웨어 - 개발 및 창조를 위한 워크스테이션 / 
		설치 대상 - 표준 지역 디스크 두번 클릭 - 파티션 직접 설정 - 다음 - swap추가해서 2g - / 로 추가하면 나머지 18g 자동 설정 / 
		Root 암호 - bigdata 로 설정 /
		유저 만들기 - 전부 hadoop으로 설정 - 관리자로 만들기 체크 안함
21.10.14 수정//
 	4) 머신 복제 - 반드시 Power off 한 이후에 복사+붙여넣기 할 것
	5) 하둡 머신 클러스터링
		- 4대를 연결(주로 머신1에서 ssh통신을 이용해서 원격으로 접속해서 작업)
		  ssh ip주소 
		  ssh(Secure Shell): 텔넷과 비슷한 개념 / 서버와 클라이언트간의 텍스트 기반으로 통신
		- 4대를 식별하기 위해서 host명 변경
		  hostnamectl set-hostname 변경하고 싶은 호스트명
		- 방화벽 해제
		  systemctl disable firewalld
				---------
				 방화벽 시스템
			// 한번에 하기 - ssh ip주소 "systemctl disable firewalld"
		- 도메인 등록 // ip로 접근하기 불편함
		  ssh통신을 하기 위해 hadoop01에서 나머지 머신을 접근
		  ip로 접근하기 불편하므로 호스트명을 등록해서 작업
		  /ect/hosts 파일 수정
			왼쪽 상단 - 위치 - 컴퓨터 - hosts 검색 - hosts파일 오른쪽 클릭하여 '텍스트 편집기로 열기 선택' - 내용 전부 지우고 'ip주소 이름' 4대 전부 입력
			다시 터미널에서 /etc/init.d/network restart
		// 원격지로 복사하기
			scp /etc/hosts root@hadoop02:/etc/hosts

		- su - hadoop // hadoop계정으로 이동함
			[root@hadoop01 ~]$ : 하둡계정
		- su - //관리자 root 계정으로 이동
			[root@hadoop01 ~]# : 하둡계정
		- ~ : 해당 계정의 홈 디렉토리. 모든 계정은 자신만의 홈디렉토리를 갖는다.
			- root계정: /root
			- 나머지 계정: /home/계정명
	//파일 설명
		/home: 사용자 계정의 홈 디렉토리
		/boot: 부팅에 필요한 각종 설정파일이 위치하는 곳
		/root: root게정의 홈 디렉토리
		/bin: 리눅스에서 사용할 수 있는 shell명령어가 위치하는 곳
		/sbin: 시스템 관리를 위한 명령어
		/etc: 시스템 관리를 위한 설정 파일이 위치하는 디렉토리
			- 사용자 정보, 파일 시스템 정보, 네트워크 정보
		/tmp: 시스템이 작업 중 사용하는 임시 폴더
		/var: tmp와 유사. 보통 로그 파일이 위치
		/lib: 공통 라이브러리 파일이 저장되는 곳
		/dev: 장치가 위치하는 디렉토리 - 리눅스는 모든 장치를 파일로 인식
		/usr: 윈도우의 program files와 동일


	hadoop설치
	[프로그램 설치]
		1) 자바 설치
			- jdk설치
		2) 하둡 설치
			- 1.x
			- 2.x
			- 3.x
[21.10.15 수정]	3) ssh프로토콜로 암호통신을 할 수 있도록 설정하기
			- 하둡 내부에서 ssh통신을 하기 대문에 통신할 수 있도록
		3) 하둡 설정

ssh-keygen -t rsa
우측 상단 선3개 버튼 눌러서 숨긴 파일 보이기
ssh-copy-id -i id_rsa.pub hadoop@hadoop02
wget https://archive.apache.org/dist/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz
-----
다운로드명령어 // 그냥 다운시 root소유로 다운이 되기 때문에 터미널에서 해당 명령어를 실행해준다.
tar -zxvf hadoop-파일명 // zip파일 x


1. /home/hadoop/hadoop-1.2.1/conf/ 폴더에서 설정파일 확인
2. 수정하고 세 대의 머신에 scp로 copy
3. hadoop을 다시 start-all.sh
4. 데몬 확인(jps)
	- 모두 동일하게 실행
		1번 - namenode.jobtracker
		2번 - secondarynamenode, datanode, tasktracker
		3번, 4번 - datanode, tasktracker

5. 1번부터 4번가지 실행 후 데몬이 정상 실행되지 않는 경우
6. 1번 머신의 hadoop계정에서 hadoop-data디렉토리 mkdir
7. namenode 초기화
8. start-all.sh 후 jps로 데몬 확인



		4) 하둡 프로그래밍
			- hdfs
			- mapreduce
			- 고급 프로그래밍(Customizing)
		5) 하둡에코시스템 설치 후 테스트
			- flume
			- sqoop
			- hive
			- pig
			- mahout
		6) R

	hadoop EcoSystem(하둡과 연관된 프로그램 설치하고 사용)
	: scoop, flume, hive, pig
- 원하는 작업을 할 수 있도록 프로그래밍
- 분석결과 활용(R, Mahout)
                       --------- 
		추천시스템